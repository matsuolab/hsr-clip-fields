{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6aa8354-6fec-4b7b-be0e-432b571fa354",
   "metadata": {},
   "source": [
    "# 3. Training a CLIP-Field\n",
    "\n",
    "In this tutorial, we are going to create a CLIP-Field from our saved data. CLIP-Field is an implicit neural field that maps from 3D XYZ coordinates to higher dimensional representations such as CLIP visual features and Sentence-BERT semantic embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678eca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "from typing import Dict, Union\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import wandb\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522b1c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from dataloaders import (\n",
    "    R3DSemanticDataset,\n",
    "    DeticDenseLabelledDataset,\n",
    "    ClassificationExtractor,\n",
    ")\n",
    "from misc import ImplicitDataparallel\n",
    "from grid_hash_model import GridCLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd413f6",
   "metadata": {},
   "source": [
    "## Load the data and create a model\n",
    "\n",
    "Now, we will set up the constants and create the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2432180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the constants\n",
    "\n",
    "SAVE_DIRECTORY = \"../clip_implicit_model\"\n",
    "DEVICE = \"cuda\"\n",
    "IMAGE_TO_LABEL_CLIP_LOSS_SCALE = 1.0\n",
    "LABEL_TO_IMAGE_LOSS_SCALE = 1.0\n",
    "EXP_DECAY_COEFF = 0.5\n",
    "SAVE_EVERY = 5\n",
    "METRICS = {\n",
    "    \"accuracy\": torchmetrics.Accuracy,\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 11000\n",
    "NUM_WORKERS = 10\n",
    "\n",
    "CLIP_MODEL_NAME = \"ViT-B/32\"\n",
    "SBERT_MODEL_NAME = \"all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f5fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and create the dataloader created in the previous tutorial notebook\n",
    "\n",
    "training_data = torch.load(\"../detic_labeled_dataset.pt\")\n",
    "max_coords, _ = training_data._label_xyz.max(dim=0)\n",
    "min_coords, _ = training_data._label_xyz.min(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e96bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model\n",
    "\n",
    "label_model = GridCLIPModel(\n",
    "    image_rep_size=training_data[0][\"clip_image_vector\"].shape[-1],\n",
    "    text_rep_size=training_data[0][\"clip_vector\"].shape[-1],\n",
    "    mlp_depth=1,\n",
    "    mlp_width=600,\n",
    "    log2_hashmap_size=20,\n",
    "    num_levels=18,\n",
    "    level_dim=8,\n",
    "    per_level_scale=2,\n",
    "    max_coords=max_coords,\n",
    "    min_coords=min_coords,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f6f4d",
   "metadata": {},
   "source": [
    "## Training and evaulation code\n",
    "\n",
    "Now, we will set up the training and the evaluation code. We will train the model to predict the CLIP/SBert features from the 3D coordinates with a contrastive loss. For evaluation, we will measure the zero-shot label accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea5bd590",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def zero_shot_eval(\n",
    "    classifier: ClassificationExtractor, \n",
    "    predicted_label_latents: torch.Tensor, \n",
    "    predicted_image_latents: torch.Tensor, \n",
    "    language_label_index: torch.Tensor, \n",
    "    metric_calculators: Dict[str, Dict[str, torchmetrics.Metric]]\n",
    "):\n",
    "    \"\"\"Evaluate the model on the zero-shot classification task.\"\"\"\n",
    "    class_probs = classifier.calculate_classifications(\n",
    "        model_text_features=predicted_label_latents,\n",
    "        model_image_features=predicted_image_latents,\n",
    "    )\n",
    "    # Now figure out semantic accuracy and loss.\n",
    "    # Semseg mask is necessary for the boundary case where all the points in the batch are \"unlabeled\"\n",
    "    semseg_mask = torch.logical_and(\n",
    "        language_label_index != -1,\n",
    "        language_label_index < classifier.total_label_classes,\n",
    "    ).squeeze(-1)\n",
    "    if not torch.any(semseg_mask):\n",
    "        classification_loss = torch.zeros_like(semseg_mask).mean(dim=-1)\n",
    "    else:\n",
    "        # Figure out the right classes.\n",
    "        masked_class_prob = class_probs[semseg_mask]\n",
    "        masked_labels = language_label_index[semseg_mask].squeeze(-1).long()\n",
    "        classification_loss = F.cross_entropy(\n",
    "            torch.log(masked_class_prob),\n",
    "            masked_labels,\n",
    "        )\n",
    "        if metric_calculators.get(\"semantic\"):\n",
    "            for _, calculators in metric_calculators[\"semantic\"].items():\n",
    "                _ = calculators(masked_class_prob, masked_labels)\n",
    "    return classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aeb16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    clip_train_loader: DataLoader,\n",
    "    labelling_model: Union[GridCLIPModel, ImplicitDataparallel],\n",
    "    optim: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    classifier: ClassificationExtractor,\n",
    "    device: Union[str, torch.device] = DEVICE,\n",
    "    exp_decay_coeff: float = EXP_DECAY_COEFF,\n",
    "    image_to_label_loss_ratio: float = IMAGE_TO_LABEL_CLIP_LOSS_SCALE,\n",
    "    label_to_image_loss_ratio: float = LABEL_TO_IMAGE_LOSS_SCALE,\n",
    "    disable_tqdm: bool = False,\n",
    "    metric_calculators: Dict[str, Dict[str, torchmetrics.Metric]] = {},\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    label_loss = 0\n",
    "    image_loss = 0\n",
    "    classification_loss = 0\n",
    "    total_samples = 0\n",
    "    total_classification_loss = 0\n",
    "    labelling_model.train()\n",
    "    total = len(clip_train_loader)\n",
    "    for clip_data_dict in tqdm.tqdm(\n",
    "        clip_train_loader,\n",
    "        total=total,\n",
    "        disable=disable_tqdm,\n",
    "        desc=f\"Training epoch {epoch}\",\n",
    "    ):\n",
    "        xyzs = clip_data_dict[\"xyz\"].to(device)\n",
    "        clip_labels = clip_data_dict[\"clip_vector\"].to(device)\n",
    "        clip_image_labels = clip_data_dict[\"clip_image_vector\"].to(device)\n",
    "        image_weights = torch.exp(-exp_decay_coeff * clip_data_dict[\"distance\"]).to(\n",
    "            device\n",
    "        )\n",
    "        label_weights = clip_data_dict[\"semantic_weight\"].to(device)\n",
    "        image_label_index: torch.Tensor = (\n",
    "            clip_data_dict[\"img_idx\"].to(device).reshape(-1, 1)\n",
    "        )\n",
    "        language_label_index: torch.Tensor = (\n",
    "            clip_data_dict[\"label\"].to(device).reshape(-1, 1)\n",
    "        )\n",
    "\n",
    "        (predicted_label_latents, predicted_image_latents) = labelling_model(xyzs)\n",
    "        # Calculate the loss from the image to label side.\n",
    "        batch_size = len(image_label_index)\n",
    "        image_label_mask: torch.Tensor = (\n",
    "            image_label_index != image_label_index.t()\n",
    "        ).float() + torch.eye(batch_size, device=device)\n",
    "        language_label_mask: torch.Tensor = (\n",
    "            language_label_index != language_label_index.t()\n",
    "        ).float() + torch.eye(batch_size, device=device)\n",
    "\n",
    "        # For logging purposes, keep track of negative samples per point.\n",
    "        image_label_mask.requires_grad = False\n",
    "        language_label_mask.requires_grad = False\n",
    "        contrastive_loss_labels = labelling_model.compute_loss(\n",
    "            predicted_label_latents,\n",
    "            clip_labels,\n",
    "            label_mask=language_label_mask,\n",
    "            weights=label_weights,\n",
    "        )\n",
    "        contrastive_loss_images = labelling_model.compute_loss(\n",
    "            predicted_image_latents,\n",
    "            clip_image_labels,\n",
    "            label_mask=image_label_mask,\n",
    "            weights=image_weights,\n",
    "        )\n",
    "        del (\n",
    "            image_label_mask,\n",
    "            image_label_index,\n",
    "            language_label_mask,\n",
    "        )\n",
    "\n",
    "        # Mostly for evaluation purposes, calculate the classification loss.\n",
    "        classification_loss = zero_shot_eval(\n",
    "            classifier, predicted_label_latents, predicted_image_latents, language_label_index, metric_calculators\n",
    "        )\n",
    "\n",
    "        contrastive_loss = (\n",
    "            image_to_label_loss_ratio * contrastive_loss_images\n",
    "            + label_to_image_loss_ratio * contrastive_loss_labels\n",
    "        )\n",
    "\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        contrastive_loss.backward()\n",
    "        optim.step()\n",
    "        # Clip the temperature term for stability\n",
    "        labelling_model.temperature.data = torch.clamp(\n",
    "            labelling_model.temperature.data, max=np.log(100.0)\n",
    "        )\n",
    "        label_loss += contrastive_loss_labels.detach().cpu().item()\n",
    "        image_loss += contrastive_loss_images.detach().cpu().item()\n",
    "        total_classification_loss += classification_loss.detach().cpu().item()\n",
    "        total_loss += contrastive_loss.detach().cpu().item()\n",
    "        total_samples += 1\n",
    "\n",
    "    to_log = {\n",
    "        \"train_avg/contrastive_loss_labels\": label_loss / total_samples,\n",
    "        \"train_avg/contrastive_loss_images\": image_loss / total_samples,\n",
    "        \"train_avg/semseg_loss\": total_classification_loss / total_samples,\n",
    "        \"train_avg/loss_sum\": total_loss / total_samples,\n",
    "        \"train_avg/labelling_temp\": torch.exp(labelling_model.temperature.data.detach())\n",
    "        .cpu()\n",
    "        .item(),\n",
    "    }\n",
    "    for metric_dict in metric_calculators.values():\n",
    "        for metric_name, metric in metric_dict.items():\n",
    "            try:\n",
    "                to_log[f\"train_avg/{metric_name}\"] = (\n",
    "                    metric.compute().detach().cpu().item()\n",
    "                )\n",
    "            except RuntimeError as e:\n",
    "                to_log[f\"train_avg/{metric_name}\"] = 0.0\n",
    "            metric.reset()\n",
    "    wandb.log(to_log)\n",
    "    logging.debug(pprint.pformat(to_log, indent=4, width=1))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a84d1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(\n",
    "    labelling_model: Union[ImplicitDataparallel, GridCLIPModel],\n",
    "    optim: torch.optim.Optimizer,\n",
    "    epoch: int,\n",
    "    save_directory: str = SAVE_DIRECTORY,\n",
    "    saving_dataparallel: bool = False,\n",
    "):\n",
    "    if saving_dataparallel:\n",
    "        to_save = labelling_model.module\n",
    "    else:\n",
    "        to_save = labelling_model\n",
    "    state_dict = {\n",
    "        \"model\": to_save.state_dict(),\n",
    "        \"optim\": optim.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    torch.save(\n",
    "        state_dict,\n",
    "        f\"{save_directory}/implicit_scene_label_model_latest.pt\",\n",
    "    )\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290e034a",
   "metadata": {},
   "source": [
    "## Set up the auxilary classes\n",
    "\n",
    "Like zero-shot classifier, dataloader, evaluators, optimizer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5492e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-10-11 10:25:47,753 - SentenceTransformer - Load pretrained SentenceTransformer: all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7d7a73c9cf46548f6b3c8bafebaa15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_classifier = ClassificationExtractor(\n",
    "    clip_model_name=CLIP_MODEL_NAME,\n",
    "    sentence_model_name=SBERT_MODEL_NAME,\n",
    "    class_names=training_data._all_classes,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "541af79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our metrics on this dataset.\n",
    "train_metric_calculators = {}\n",
    "train_class_count = {\"semantic\": train_classifier.total_label_classes}\n",
    "average_style = [\"micro\", \"macro\", \"weighted\"]\n",
    "for classes, counts in train_class_count.items():\n",
    "    train_metric_calculators[classes] = {}\n",
    "    for metric_name, metric_cls in METRICS.items():\n",
    "        for avg in average_style:\n",
    "            if \"accuracy\" in metric_name:\n",
    "                new_metric = metric_cls(\n",
    "                    num_classes=counts, average=avg, multiclass=True\n",
    "                ).to(DEVICE)\n",
    "                train_metric_calculators[classes][\n",
    "                    f\"{classes}_{metric_name}_{avg}\"\n",
    "                ] = new_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79729e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No dataparallel for now\n",
    "batch_multiplier = 1\n",
    "\n",
    "clip_train_loader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=batch_multiplier * BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "logging.debug(f\"Total train dataset sizes: {len(training_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa1089ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "\n",
    "optim = torch.optim.Adam(\n",
    "    label_model.parameters(),\n",
    "    lr=1e-4,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.003,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b73a4",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Now we run our training loop and save the model occassionally. We ran this for 5 epochs just to validate everything is working properly, but to train a full model you should train it for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac06a5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - 2022-10-11 10:25:49,099 - jupyter - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098342898963460695243efa0688e394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.03333655993143717, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mahi/code/clip-fields/demo/wandb/run-20221011_102550-j12j175e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mahi/clipfields/runs/j12j175e\" target=\"_blank\">gallant-river-1</a></strong> to <a href=\"https://wandb.ai/mahi/clipfields\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"clipfields\",\n",
    ")\n",
    "# Set the extra parameters.\n",
    "wandb.config.web_labelled_points = len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb3671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.25it/s]\n",
      "Training epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.34it/s]\n",
      "Training epoch 2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.33it/s]\n",
      "Training epoch 3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.35it/s]\n",
      "Training epoch 4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.34it/s]\n",
      "Training epoch 5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:34<00:00,  7.33it/s]\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Just to reduce excessive logging from sbert\n",
    "\n",
    "epoch = 0\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "while epoch <= NUM_EPOCHS:\n",
    "    train(\n",
    "        clip_train_loader,\n",
    "        label_model,\n",
    "        optim,\n",
    "        epoch,\n",
    "        train_classifier,\n",
    "        metric_calculators=train_metric_calculators,\n",
    "    )\n",
    "    epoch += 1\n",
    "    if epoch % SAVE_EVERY == 0:\n",
    "        save(label_model, optim, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ff760-c0eb-4ace-b1e7-fc92a03c62be",
   "metadata": {},
   "source": [
    "This is saved already in `../clip_implicit_model`, so we don't have to save our trained model again. You can see the run data on [Weights and biases](https://wandb.ai/mahi/clipfields/runs/j12j175e). On our next tutorial episode, we will evaluate our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "db1f79a380f3c855f0bd6a6f1ad2b80d271c32dcf53749ce06204dd6e0a63f81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
